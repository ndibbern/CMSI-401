{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106d6f0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17000, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = ['Zonal_Winds', 'Meridional_Winds', 'Humidity', 'Air_Temp', 'Sea_Surface_Temp']\n",
    "train_data = pd.read_csv('../../Cleaning-the-data/tao-cleaned.csv')[rows]\n",
    "\n",
    "train_data = (train_data - train_data.mean())/train_data.std()\n",
    "train_data = train_data.values[:17000]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence,self).__init__()\n",
    "        self.lstm1 = nn.LSTM(5,50, 3)\n",
    "        self.lstm2 = nn.LSTM(50,1, 2)\n",
    "        \n",
    " \n",
    "    def forward(self, seq):\n",
    "        lstm1_out, _ = self.lstm1(seq, None)\n",
    "        lstm2_out, _ = self.lstm2(lstm1_out, None)\n",
    "        return lstm2_out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "#         self.linear1 = nn.Linear(5, 1)\n",
    "        self.lstm1 = nn.LSTMCell(5, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "#             input_t = self.linear1(input_t)\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lirnli.wordpress.com/2017/09/01/simple-pytorch-rnn-examples/\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence,self).__init__()\n",
    "        self.lstm1 = nn.LSTM(5,64, 2)\n",
    "        self.lstm2 = nn.LSTM(64,1)\n",
    " \n",
    "    def forward(self,seq, hc = None):\n",
    "        out = []\n",
    "        if hc == None:\n",
    "            hc1, hc2 = None, None\n",
    "        else:\n",
    "            hc1, hc2 = hc\n",
    "        \n",
    "        for X in seq.chunk(seq.size(1),dim=0):\n",
    "#             print(X)\n",
    "            tmp, hc1 = self.lstm1(X, hc1)\n",
    "            X_in, hc2 = self.lstm2(tmp, hc2)\n",
    "            out.append(X_in)\n",
    "        return torch.stack(out).squeeze(1),(hc1,hc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0080],\n",
       "           [-0.0091]],\n",
       " \n",
       "          [[-0.0231],\n",
       "           [-0.0261]],\n",
       " \n",
       "          [[-0.0422],\n",
       "           [-0.0438]],\n",
       " \n",
       "          [[-0.0580],\n",
       "           [-0.0592]],\n",
       " \n",
       "          [[-0.0712],\n",
       "           [-0.0694]],\n",
       " \n",
       "          [[-0.0776],\n",
       "           [-0.0755]],\n",
       " \n",
       "          [[-0.0809],\n",
       "           [-0.0797]],\n",
       " \n",
       "          [[-0.0851],\n",
       "           [-0.0805]]],\n",
       " \n",
       " \n",
       "         [[[-0.0845],\n",
       "           [-0.0789]],\n",
       " \n",
       "          [[-0.0808],\n",
       "           [-0.0749]],\n",
       " \n",
       "          [[-0.0751],\n",
       "           [-0.0700]],\n",
       " \n",
       "          [[-0.0691],\n",
       "           [-0.0746]],\n",
       " \n",
       "          [[-0.0734],\n",
       "           [-0.0839]],\n",
       " \n",
       "          [[-0.0828],\n",
       "           [-0.0955]],\n",
       " \n",
       "          [[-0.0939],\n",
       "           [-0.1056]],\n",
       " \n",
       "          [[-0.1042],\n",
       "           [-0.1144]]]], dtype=torch.float64, grad_fn=<SqueezeBackward1>),\n",
       " ((tensor([[[ 0.0413,  0.1396,  0.1010,  0.0795,  0.0717,  0.1796,  0.1217,\n",
       "             -0.0779,  0.1609,  0.0360,  0.1251,  0.0682,  0.0109,  0.0449,\n",
       "             -0.1989, -0.1712, -0.2070,  0.0326,  0.0662,  0.1185,  0.0263,\n",
       "              0.0704, -0.0472, -0.0793,  0.0336, -0.0935, -0.2970,  0.1003,\n",
       "              0.0907,  0.2262, -0.0926,  0.1061,  0.0878, -0.0298, -0.1947,\n",
       "             -0.1035, -0.0158, -0.1585,  0.0111, -0.0891,  0.2372,  0.0294,\n",
       "             -0.2104,  0.0533, -0.2132, -0.2193,  0.1085,  0.2157,  0.2191,\n",
       "             -0.1730,  0.0627,  0.0874, -0.1455,  0.0260, -0.0246,  0.1837,\n",
       "             -0.1991, -0.2553, -0.1957, -0.0766,  0.0291,  0.1096,  0.0028,\n",
       "              0.0142],\n",
       "            [ 0.0536,  0.1481,  0.0813,  0.0906,  0.0600,  0.1897,  0.1168,\n",
       "             -0.0764,  0.1539,  0.0315,  0.1134,  0.0685, -0.0053,  0.0508,\n",
       "             -0.1939, -0.1961, -0.2112,  0.0404,  0.0815,  0.1318,  0.0265,\n",
       "              0.0911, -0.0213, -0.0650,  0.0350, -0.0992, -0.3164,  0.0958,\n",
       "              0.0868,  0.2178, -0.0785,  0.1017,  0.0848, -0.0432, -0.1817,\n",
       "             -0.1050, -0.0171, -0.1509,  0.0235, -0.0754,  0.2624,  0.0424,\n",
       "             -0.2235,  0.0716, -0.2395, -0.2213,  0.0969,  0.2298,  0.2492,\n",
       "             -0.1656,  0.0746,  0.0827, -0.1562,  0.0228, -0.0106,  0.1886,\n",
       "             -0.1968, -0.2783, -0.1984, -0.0826,  0.0325,  0.0950,  0.0166,\n",
       "              0.0362]],\n",
       "   \n",
       "           [[ 0.0355,  0.0873, -0.0029,  0.0178,  0.0189,  0.0703,  0.0094,\n",
       "              0.1080, -0.0070, -0.0258,  0.0731,  0.0285,  0.1050, -0.0381,\n",
       "              0.0429,  0.0076, -0.0386, -0.0557, -0.0908,  0.0801,  0.0000,\n",
       "              0.0447,  0.0962, -0.1391, -0.0838, -0.0936,  0.0489,  0.0209,\n",
       "             -0.0429,  0.1023,  0.0489,  0.0028, -0.0759, -0.0287,  0.0493,\n",
       "              0.0403, -0.0165,  0.0218,  0.0241, -0.0497,  0.0579, -0.0412,\n",
       "              0.0053, -0.0774,  0.0672,  0.0188, -0.0222,  0.0461,  0.1013,\n",
       "             -0.1058, -0.0763, -0.0105,  0.0575, -0.0396, -0.0793,  0.1400,\n",
       "             -0.0481,  0.0435, -0.0392, -0.0010,  0.0208, -0.0835, -0.0658,\n",
       "             -0.1659],\n",
       "            [ 0.0352,  0.0904,  0.0018,  0.0152,  0.0295,  0.0670,  0.0135,\n",
       "              0.1101, -0.0096, -0.0250,  0.0791,  0.0309,  0.1079, -0.0421,\n",
       "              0.0451,  0.0060, -0.0455, -0.0630, -0.0908,  0.0880, -0.0007,\n",
       "              0.0417,  0.1000, -0.1444, -0.0874, -0.0978,  0.0587,  0.0220,\n",
       "             -0.0416,  0.0981,  0.0463,  0.0073, -0.0788, -0.0192,  0.0530,\n",
       "              0.0442, -0.0160,  0.0216,  0.0231, -0.0557,  0.0554, -0.0424,\n",
       "              0.0036, -0.0852,  0.0737,  0.0189, -0.0318,  0.0516,  0.1068,\n",
       "             -0.1024, -0.0766, -0.0078,  0.0619, -0.0409, -0.0873,  0.1442,\n",
       "             -0.0504,  0.0503, -0.0420, -0.0060,  0.0217, -0.0838, -0.0719,\n",
       "             -0.1616]]], dtype=torch.float64, grad_fn=<ViewBackward>),\n",
       "   tensor([[[ 0.0993,  0.2898,  0.2601,  0.1439,  0.1767,  0.4895,  0.2423,\n",
       "             -0.1292,  0.2853,  0.0780,  0.2269,  0.1035,  0.0194,  0.1015,\n",
       "             -0.3250, -0.3048, -0.6690,  0.0509,  0.1708,  0.3844,  0.0512,\n",
       "              0.1098, -0.1079, -0.1620,  0.0495, -0.2734, -0.6620,  0.2337,\n",
       "              0.1752,  0.4968, -0.2068,  0.3154,  0.1522, -0.0694, -0.5006,\n",
       "             -0.2073, -0.0360, -0.3547,  0.0224, -0.1556,  0.4773,  0.0480,\n",
       "             -0.4225,  0.1053, -0.3794, -0.4905,  0.2436,  0.3710,  0.5372,\n",
       "             -0.3170,  0.1633,  0.1903, -0.3856,  0.0546, -0.0446,  0.3316,\n",
       "             -0.4055, -0.4186, -0.3491, -0.1599,  0.0666,  0.1921,  0.0047,\n",
       "              0.0309],\n",
       "            [ 0.1302,  0.3096,  0.2080,  0.1625,  0.1504,  0.5111,  0.2320,\n",
       "             -0.1253,  0.2732,  0.0678,  0.1998,  0.1020, -0.0093,  0.1137,\n",
       "             -0.3110, -0.3572, -0.6752,  0.0622,  0.2130,  0.4462,  0.0506,\n",
       "              0.1427, -0.0465, -0.1336,  0.0503, -0.3010, -0.7264,  0.2273,\n",
       "              0.1694,  0.4795, -0.1725,  0.3089,  0.1466, -0.1014, -0.4766,\n",
       "             -0.2143, -0.0380, -0.3388,  0.0471, -0.1323,  0.5327,  0.0694,\n",
       "             -0.4596,  0.1399, -0.4185, -0.5002,  0.2166,  0.3989,  0.6174,\n",
       "             -0.3052,  0.2019,  0.1794, -0.4035,  0.0475, -0.0191,  0.3364,\n",
       "             -0.3999, -0.4613, -0.3528, -0.1754,  0.0730,  0.1632,  0.0283,\n",
       "              0.0781]],\n",
       "   \n",
       "           [[ 0.0698,  0.1937, -0.0062,  0.0336,  0.0372,  0.1547,  0.0183,\n",
       "              0.2117, -0.0132, -0.0488,  0.1332,  0.0561,  0.1847, -0.0753,\n",
       "              0.0949,  0.0154, -0.0802, -0.1046, -0.1649,  0.1593,  0.0000,\n",
       "              0.0931,  0.1934, -0.2701, -0.1561, -0.1918,  0.0915,  0.0429,\n",
       "             -0.0787,  0.1877,  0.1055,  0.0061, -0.1688, -0.0572,  0.0930,\n",
       "              0.0747, -0.0308,  0.0440,  0.0450, -0.0963,  0.1165, -0.0768,\n",
       "              0.0109, -0.1695,  0.1470,  0.0384, -0.0391,  0.0949,  0.2414,\n",
       "             -0.2144, -0.1422, -0.0231,  0.1095, -0.0886, -0.1421,  0.2695,\n",
       "             -0.1057,  0.0924, -0.0811, -0.0021,  0.0402, -0.1516, -0.1422,\n",
       "             -0.3303],\n",
       "            [ 0.0691,  0.2017,  0.0038,  0.0288,  0.0582,  0.1478,  0.0263,\n",
       "              0.2169, -0.0181, -0.0474,  0.1437,  0.0609,  0.1895, -0.0825,\n",
       "              0.0997,  0.0120, -0.0953, -0.1191, -0.1649,  0.1745, -0.0014,\n",
       "              0.0868,  0.2015, -0.2816, -0.1621, -0.2014,  0.1102,  0.0450,\n",
       "             -0.0766,  0.1792,  0.1009,  0.0160, -0.1758, -0.0383,  0.0998,\n",
       "              0.0819, -0.0299,  0.0436,  0.0431, -0.1079,  0.1122, -0.0790,\n",
       "              0.0073, -0.1875,  0.1617,  0.0386, -0.0559,  0.1066,  0.2542,\n",
       "             -0.2080, -0.1422, -0.0173,  0.1180, -0.0915, -0.1553,  0.2782,\n",
       "             -0.1118,  0.1061, -0.0867, -0.0130,  0.0422, -0.1524, -0.1564,\n",
       "             -0.3223]]], dtype=torch.float64, grad_fn=<ViewBackward>)),\n",
       "  (tensor([[[-0.1042],\n",
       "            [-0.1144]]], dtype=torch.float64, grad_fn=<ViewBackward>),\n",
       "   tensor([[[-0.5846],\n",
       "            [-0.6427]]], dtype=torch.float64, grad_fn=<ViewBackward>))))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = Sequence()\n",
    "seq.double()\n",
    "test_data = torch.from_numpy(train_data[:32]).view(-1, 2, 5)\n",
    "seq(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lr = 0.1\n",
    "optimizer = optim.Adam(seq.parameters(), lr=lr)\n",
    "# optimizer = optim.LBFGS(seq.parameters(), lr=lr)\n",
    "seq_size = 128\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t loss= 1.4754435234243772 \t error=72.91% percent\n",
      "epoch= 5 \t loss= 1.6262214025295234 \t error=71.34% percent\n",
      "epoch= 10 \t loss= 1.4581966626345175 \t error=70.63% percent\n",
      "epoch= 15 \t loss= 1.4723582172141254 \t error=73.02% percent\n",
      "epoch= 20 \t loss= 1.4538356176133107 \t error=71.27% percent\n",
      "epoch= 25 \t loss= 1.3638736767095532 \t error=68.03% percent\n",
      "epoch= 30 \t loss= 1.4617069275700316 \t error=73.23% percent\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    running_loss = 0\n",
    "    running_error = 0\n",
    "    iterations = 0\n",
    "    for i in range(0, 12800, seq_size):\n",
    "        data = train_data[i:i+seq_size+1]\n",
    "        xs = torch.from_numpy(data[:-1]).view(-1, batch_size, 5)\n",
    "        ys = torch.from_numpy(data[1:][:,-1]).view(-1, batch_size)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        lstm_out, _ = seq(xs)\n",
    "        loss = criterion(lstm_out.view(1, -1), ys.view(1, -1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iterations += 1\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss.item()\n",
    "            error = utils.get_error(lstm_out.view(1, -1), ys.view(1, -1))\n",
    "            running_error += error \n",
    "\n",
    "    # once the epoch is finished we divide the \"running quantities\"\n",
    "    # by the number of batches\n",
    "    if epoch % 5 == 0:\n",
    "        total_loss = running_loss/iterations\n",
    "        total_error = running_error/iterations\n",
    "        print('epoch=',epoch, '\\t loss=', total_loss, '\\t error={:2.2%}'.format(total_error) ,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
